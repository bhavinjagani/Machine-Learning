{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8100558659217877\n",
      "Random Forest Accuracy: 0.7597765363128491\n",
      "AdaBoost Accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion='gini', max_depth=None, min_sample_split=2, min_samples_leaf=1):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs) for inputs in X])\n",
    "\n",
    "    def _gini(self, y):\n",
    "        y = np.array(y, dtype=int)\n",
    "        y = y - np.min(y)  # Ensure non-negative labels\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "    def _split(self, X, y, index, threshold):\n",
    "        left_mask = X[:, index] < threshold\n",
    "        right_mask = X[:, index] >= threshold\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def _information_gain(self, y, y_left, y_right, criterion):\n",
    "        p = len(y_left) / len(y)\n",
    "        if criterion == 'gini':\n",
    "            return self._gini(y) - (p * self._gini(y_left) + (1 - p) * self._gini(y_right))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion.\")\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_index, best_threshold = None, None\n",
    "\n",
    "        for index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, index])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = self._split(X, y, index, threshold)\n",
    "                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, y_left, y_right, self.criterion)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_index = index\n",
    "                    best_threshold = threshold\n",
    "        return best_index, best_threshold\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_labels = len(np.unique(y))\n",
    "\n",
    "        if (depth >= self.max_depth or num_labels == 1 or num_samples < self.min_sample_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'leaf': True, 'value': leaf_value}\n",
    "\n",
    "        index, threshold = self._best_split(X, y)\n",
    "        if index is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'leaf': True, 'value': leaf_value}\n",
    "\n",
    "        X_left, X_right, y_left, y_right = self._split(X, y, index, threshold)\n",
    "        left_subtree = self._grow_tree(X_left, y_left, depth + 1)\n",
    "        right_subtree = self._grow_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return {\n",
    "            'leaf': False,\n",
    "            'index': index,\n",
    "            'threshold': threshold,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree\n",
    "        while not node['leaf']:\n",
    "            if inputs[node['index']] < node['threshold']:\n",
    "                node = node['left']\n",
    "            else:\n",
    "                node = node['right']\n",
    "        return node['value']\n",
    "    \n",
    "    \n",
    "class RandomForest:\n",
    "    def __init__(self, base_classifier, num_trees=10, min_features=2):\n",
    "        self.base_classifier = base_classifier\n",
    "        self.num_trees = num_trees\n",
    "        self.min_features = min_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.trees = []\n",
    "\n",
    "        for _ in range(self.num_trees):\n",
    "            # Bagging: Sample with replacement\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_subset, y_subset = X[indices], y[indices]\n",
    "\n",
    "            # Feature selection\n",
    "            feature_indices = np.random.choice(n_features, min(self.min_features, n_features), replace=False)\n",
    "            X_subset = X_subset[:, feature_indices]\n",
    "\n",
    "            # Train a tree\n",
    "            tree = self.base_classifier\n",
    "            tree_params = {k: v for k, v in self.base_classifier.__dict__.items() if k != 'tree'}\n",
    "            tree_instance = self.base_classifier.__class__(**tree_params)\n",
    "            tree_instance.fit(X_subset, y_subset)\n",
    "            self.trees.append((tree_instance, feature_indices))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((len(X), len(self.trees)))\n",
    "        for i, (tree, feature_indices) in enumerate(self.trees):\n",
    "            X_subset = X[:, feature_indices]\n",
    "            predictions[:, i] = tree.predict(X_subset)\n",
    "        return np.array([Counter(row).most_common(1)[0][0] for row in predictions])\n",
    "    \n",
    "    \n",
    "class AdaBoost:\n",
    "    def __init__(self, weak_learner, num_learners=50, learning_rate=1.0):\n",
    "        self.weak_learner = weak_learner\n",
    "        self.num_learners = num_learners\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learners = []\n",
    "        self.learner_weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "        y = np.where(y == 0, -1, 1)  # Convert labels to {-1, 1}\n",
    "\n",
    "        for _ in range(self.num_learners):\n",
    "            # Train a weak learner\n",
    "            learner = self._create_weak_learner()\n",
    "            learner.fit(X, y)\n",
    "            predictions = learner.predict(X)\n",
    "\n",
    "            # Compute error and learner weight\n",
    "            error = np.sum(weights * (predictions != y))\n",
    "            if error > 0.5:\n",
    "                break\n",
    "            learner_weight = self.learning_rate * 0.5 * np.log((1 - error) / error)\n",
    "\n",
    "            # Update weights\n",
    "            weights *= np.exp(-learner_weight * y * predictions)\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "            # Save learner and its weight\n",
    "            self.learners.append(learner)\n",
    "            self.learner_weights.append(learner_weight)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for learner, weight in zip(self.learners, self.learner_weights):\n",
    "            predictions += weight * learner.predict(X)\n",
    "        return np.where(predictions >= 0, 1, 0)\n",
    "\n",
    "    def _create_weak_learner(self):\n",
    "        # Create a new instance of the weak learner\n",
    "        learner_params = {k: v for k, v in self.weak_learner.__dict__.items() if k != 'tree'}\n",
    "        return self.weak_learner.__class__(**learner_params)\n",
    "    \n",
    "\n",
    "\n",
    "# Load and preprocess the Titanic dataset\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "original_train = train.copy()\n",
    "original_test = test.copy()\n",
    "full_data = [train, test]\n",
    "\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# Remove all NULLS in the Fare column\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "\n",
    "# Remove all NULLS in the Age column\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    # Next line has been improved to avoid warning\n",
    "    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n",
    "\n",
    "\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X = train.drop('Survived', axis=1)\n",
    "y = train['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# Train and evaluate Decision Tree\n",
    "dt = DecisionTree(max_depth=5)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_dt)}\")\n",
    "\n",
    "# Train and evaluate Random Forest\n",
    "rf = RandomForest(base_classifier = DecisionTree(max_depth=5), num_trees=50)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
    "\n",
    "# Train and evaluate AdaBoost\n",
    "ab = AdaBoost(weak_learner = DecisionTree(max_depth=1), num_learners=50)\n",
    "ab.fit(X_train, y_train)\n",
    "y_pred_ab = ab.predict(X_test)\n",
    "print(f\"AdaBoost Accuracy: {accuracy_score(y_test, y_pred_ab)}\")    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
