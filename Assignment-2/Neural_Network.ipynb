{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class layer: \n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError            \n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError\n",
    "\n",
    "   \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_layer(layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        # Initialize weights with small random values and biases as zeros.\n",
    "        self.w = np.random.randn(output_dim,input_dim) * 0.01\n",
    "        self.b = np.zeros(output_dim)\n",
    "        self.input = None\n",
    "        # Gradients computed during backward pass\n",
    "        self.grad_w = None\n",
    "        self.grad_b = None\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.w.T) + self.b\n",
    "\n",
    "    def backward(self, delta):\n",
    "        self.grad_w = delta.T.dot(self.input)\n",
    "        self.grad_b = delta.sum(axis=0)\n",
    "        grad_input = delta.dot(self.w)  # shape (n, input_dim)\n",
    "        return grad_input\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_layer(layer):\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, input):\n",
    "       self.out = 1 / (1 + np.exp(-input))\n",
    "       return self.out\n",
    "\n",
    "    def backward(self, delta):\n",
    "        return delta * self.out * (1 - self.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanh_layer(layer):\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.out = np.tanh(input)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, delta):\n",
    "        return delta * (1 - self.out ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu_layer(layer):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad = grad_output.copy()\n",
    "        grad[self.input <= 0] = 0\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropyLoss(layer):\n",
    "    def __init__(self):\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Clip predictions to avoid log(0)\n",
    "        self.pred = np.clip(pred, 1e-7, 1 - 1e-7)\n",
    "        self.target = target\n",
    "        loss = - (target * np.log(self.pred) + (1 - target) * np.log(1 - self.pred))\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def backward(self):\n",
    "        # Compute gradient of BCE loss with respect to predictions.\n",
    "        grad = (-(self.target / self.pred) + ((1 - self.target) / (1 - self.pred))) / self.target.shape[0]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(layer):\n",
    "    def __init__(self):\n",
    "        self.pred = None\n",
    "        self.target = None\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        self.pred = pred\n",
    "        self.target = target\n",
    "        loss = np.mean((pred - target) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        grad = 2 * (self.pred - self.target) / self.target.shape[0]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Container for Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(layer):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "        return grad_output\n",
    "\n",
    "    def save_weights(self, filename):\n",
    "        # Save weights for layers that have parameters (e.g., Linear layers)\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, linear_layer):\n",
    "                weights.append({'w': layer.w, 'b': layer.b})\n",
    "            else:\n",
    "                weights.append(None)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(weights, f)\n",
    "        print(\"Weights saved to\", filename)\n",
    "\n",
    "    def load_weights(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            weights = pickle.load(f)\n",
    "        for layer, w in zip(self.layers, weights):\n",
    "            if isinstance(layer, linear_layer) and w is not None:\n",
    "                layer.w = w['w']\n",
    "                layer.b = w['b']\n",
    "        print(\"Weights loaded from\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Classification Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, loss_layer, X, y, epochs, lr):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        pred = network.forward(X)\n",
    "        loss = loss_layer.forward(pred, y)\n",
    "        losses.append(loss)\n",
    "        # Backward pass\n",
    "        grad_loss = loss_layer.backward()\n",
    "        network.backward(grad_loss)\n",
    "        # Update parameters for every Linear layer\n",
    "        for layer in network.layers:\n",
    "            if isinstance(layer, linear_layer):\n",
    "                layer.w -= lr * layer.grad_w\n",
    "                layer.b -= lr * layer.grad_b\n",
    "        # Print progress periodically\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"Epoch {}: Loss = {:.4f}\".format(epoch, loss))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Regression with Early Stopping (Taxi Trip Duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(network, loss_layer, X_train, y_train, X_val, y_val, epochs, lr, patience=30):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        # --- Training Step ---\n",
    "        pred_train = network.forward(X_train)\n",
    "        train_loss = loss_layer.forward(pred_train, y_train)\n",
    "        train_losses.append(train_loss)\n",
    "        grad_loss = loss_layer.backward()\n",
    "        network.backward(grad_loss)\n",
    "        for layer in network.layers:\n",
    "            if isinstance(layer, linear_layer):\n",
    "                layer.w -= lr * layer.grad_w\n",
    "                layer.b -= lr * layer.grad_b\n",
    "\n",
    "        # --- Validation Step ---\n",
    "        pred_val = network.forward(X_val)\n",
    "        val_loss = loss_layer.forward(pred_val, y_val)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered at epoch\", epoch)\n",
    "            break\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch {}: Train Loss = {:.4f}, Val Loss = {:.4f}\".format(epoch, train_loss, val_loss))\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration: XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_XOR():\n",
    "    X = np.array([[0,0], [0,1], [1,0], [1,1]]) # XOR input\n",
    "    Y = np.array([[0], [1], [1], [0]]) # XOR output\n",
    "\n",
    "    print(\"Training XOR network with sigmoid activation...\")\n",
    "\n",
    "\n",
    "    network_sigmoid = Sequential([linear_layer(2,2), sigmoid_layer(), linear_layer(2,1), sigmoid_layer()])\n",
    "    loss_bce = BinaryCrossEntropyLoss()\n",
    "    losses_sigmoid = train(network_sigmoid, loss_bce, X, Y, epochs=10000, lr=0.1)\n",
    "\n",
    "    preds_sigmoid = network_sigmoid.forward(X)\n",
    "    print(\"XOR Predictions (Sigmoid):\\n\", preds_sigmoid)\n",
    "    \n",
    "    # Save the trained weights\n",
    "    network_sigmoid.save_weights(\"XOR_solved.w\")\n",
    "\n",
    "\n",
    "    print(\"Training XOR network with tanh activation...\")\n",
    "    network_tanh = Sequential([linear_layer(2,2), tanh_layer(), linear_layer(2,1), sigmoid_layer()])\n",
    "    losses_tanh = train(network_tanh, loss_bce, X, Y, epochs=10000, lr=0.1)\n",
    "    preds_tanh = network_tanh.forward(X)\n",
    "    print(\"XOR Predictions (Tanh hidden activation):\\n\", preds_tanh)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(losses_sigmoid, label=\"Sigmoid\")\n",
    "    plt.plot(losses_tanh, label=\"Tanh\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"XOR Training Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration: Taxi Trip Duration Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_frame, is_training=True):\n",
    "    if is_training:\n",
    "        preprocessed_data = data_frame.dropna()\n",
    "    else:\n",
    "        preprocessed_data = data_frame  # No preprocessing for testing data\n",
    "    return preprocessed_data\n",
    "\n",
    "\n",
    "def predict_taxi_trip_duration():\n",
    "    # Load the dataset (make sure the file nyc_taxi_data.npy is in the working directory)\n",
    "    try:\n",
    "        dataset = np.load(\"nyc_taxi_data.npy\", allow_pickle=True)\n",
    "        dataset = dataset.item()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error loading taxi dataset:\", e)\n",
    "        return\n",
    "\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = dataset[\"X_train\"], dataset[\"y_train\"], dataset[\"X_test\"], dataset[\"y_test\"]\n",
    "    X_train = preprocess_data(X_train)\n",
    "    y_train = preprocess_data(y_train)\n",
    "    X_test = preprocess_data(X_test)\n",
    "    y_test = preprocess_data(y_test)\n",
    "    columns_to_drop = [\"id\", \"pickup_datetime\", \"dropoff_datetime\", \"trip_duration\", \"store_and_fwd_flag\"]\n",
    "    X_train = X_train.drop(columns=[col for col in columns_to_drop if col in X_train.columns])\n",
    "    X_test = X_test.drop(columns=[col for col in columns_to_drop if col in X_test.columns])\n",
    "\n",
    "    y_train = y_train.values.reshape(-1, 1)\n",
    "    y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "    # For this demonstration, we perform a simple normalization.\n",
    "    feature_scaler = StandardScaler()\n",
    "    X_train_norm = feature_scaler.fit_transform(X_train)\n",
    "    X_test_norm = feature_scaler.transform(X_test)\n",
    "\n",
    "           \n",
    "    \n",
    "    # Split a validation set from training data (80/20 split)\n",
    "    split = int(0.8 * X_train_norm.shape[0])\n",
    "    X_train_new, X_val = X_train_norm[:split], X_train_norm[split:]\n",
    "    y_train_new, y_val = y_train[:split], y_train[split:]\n",
    "    \n",
    "    # Here we define one example network for regression.\n",
    "    # You should experiment with at least three configurations.\n",
    "    # This network has 2 hidden layers with 10 nodes each and a linear output.\n",
    "\n",
    "    # network_configs = [\n",
    "    #     [linear_layer(X_train_new.shape[1], 10), relu_layer(), linear_layer(10, 10), relu_layer(), linear_layer(10, 1)],\n",
    "    #     [LinearLayer(X_train.shape[1], 128), TanhActivation(), LinearLayer(128, 64), TanhActivation(), LinearLayer(64, 1)],\n",
    "    #     [LinearLayer(X_train.shape[1], 256), TanhActivation(), LinearLayer(256, 128), TanhActivation(), LinearLayer(128, 64), TanhActivation(), LinearLayer(64, 1)]\n",
    "    # ]\n",
    "\n",
    "    network_reg = Sequential([\n",
    "        linear_layer(X_train_new.shape[1], 10),\n",
    "        relu_layer(),\n",
    "        linear_layer(10, 10),\n",
    "        relu_layer(),\n",
    "        linear_layer(10, 1)  # For regression, no activation (or identity) on the output.\n",
    "    ])\n",
    "     \n",
    "    mse_loss = MSELoss()\n",
    "    print(\"\\n--- Training Taxi Trip Duration Network ---\")\n",
    "    train_losses, val_losses = train_regression(network_reg, mse_loss,\n",
    "                                                X_train_new, y_train_new,\n",
    "                                                X_val, y_val,\n",
    "                                                epochs=1000, lr=0.0001, patience=20)\n",
    "    \n",
    "    # Plot training and validation loss curves\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Taxi Trip Duration Training\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss = mse_loss.forward(network_reg.forward(X_test_norm), y_test)\n",
    "    \n",
    "    # For RMSLE (Root Mean Squared Logarithmic Error) we can compute:\n",
    "    pred_test = network_reg.forward(X_test_norm)\n",
    "    # Clip predictions to avoid log issues\n",
    "    pred_test = np.clip(pred_test, 1e-7, None)\n",
    "    rmsle = np.sqrt(np.mean((np.log1p(pred_test) - np.log1p(y_test))**2))\n",
    "    print(\"Test RMSLE:\", rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_new.shape[1] 6\n",
      "\n",
      "--- Training Taxi Trip Duration Network ---\n",
      "grad_loss [[-0.00106834]\n",
      " [-0.00092552]\n",
      " [-0.00199767]\n",
      " ...\n",
      " [-0.00048942]\n",
      " [-0.0031022 ]\n",
      " [-0.00064558]]\n",
      "Epoch 0: Train Loss = 15732844.2129, Val Loss = 87989162.4327\n",
      "grad_loss [[-0.00106798]\n",
      " [-0.00092515]\n",
      " [-0.00199731]\n",
      " ...\n",
      " [-0.00048906]\n",
      " [-0.00310183]\n",
      " [-0.00064521]]\n",
      "grad_loss [[-0.00106762]\n",
      " [-0.00092479]\n",
      " [-0.00199694]\n",
      " ...\n",
      " [-0.00048869]\n",
      " [-0.00310147]\n",
      " [-0.00064485]]\n",
      "grad_loss [[-0.00106725]\n",
      " [-0.00092443]\n",
      " [-0.00199658]\n",
      " ...\n",
      " [-0.00048833]\n",
      " [-0.00310111]\n",
      " [-0.00064449]]\n",
      "grad_loss [[-0.00106689]\n",
      " [-0.00092406]\n",
      " [-0.00199622]\n",
      " ...\n",
      " [-0.00048797]\n",
      " [-0.00310074]\n",
      " [-0.00064412]]\n",
      "grad_loss [[-0.00106653]\n",
      " [-0.0009237 ]\n",
      " [-0.00199585]\n",
      " ...\n",
      " [-0.0004876 ]\n",
      " [-0.00310038]\n",
      " [-0.00064376]]\n",
      "grad_loss [[-0.00106616]\n",
      " [-0.00092334]\n",
      " [-0.00199549]\n",
      " ...\n",
      " [-0.00048724]\n",
      " [-0.00310002]\n",
      " [-0.00064339]]\n",
      "grad_loss [[-0.0010658 ]\n",
      " [-0.00092297]\n",
      " [-0.00199512]\n",
      " ...\n",
      " [-0.00048687]\n",
      " [-0.00309965]\n",
      " [-0.00064303]]\n",
      "grad_loss [[-0.00106543]\n",
      " [-0.00092261]\n",
      " [-0.00199476]\n",
      " ...\n",
      " [-0.00048651]\n",
      " [-0.00309929]\n",
      " [-0.00064267]]\n",
      "grad_loss [[-0.00106507]\n",
      " [-0.00092224]\n",
      " [-0.0019944 ]\n",
      " ...\n",
      " [-0.00048614]\n",
      " [-0.00309892]\n",
      " [-0.0006423 ]]\n",
      "grad_loss [[-0.0010647 ]\n",
      " [-0.00092188]\n",
      " [-0.00199403]\n",
      " ...\n",
      " [-0.00048578]\n",
      " [-0.00309856]\n",
      " [-0.00064194]]\n",
      "grad_loss [[-0.00106434]\n",
      " [-0.00092151]\n",
      " [-0.00199366]\n",
      " ...\n",
      " [-0.00048541]\n",
      " [-0.00309819]\n",
      " [-0.00064157]]\n",
      "grad_loss [[-0.00106397]\n",
      " [-0.00092114]\n",
      " [-0.0019933 ]\n",
      " ...\n",
      " [-0.00048505]\n",
      " [-0.00309782]\n",
      " [-0.0006412 ]]\n",
      "grad_loss [[-0.0010636 ]\n",
      " [-0.00092077]\n",
      " [-0.00199293]\n",
      " ...\n",
      " [-0.00048468]\n",
      " [-0.00309746]\n",
      " [-0.00064083]]\n",
      "grad_loss [[-0.00106323]\n",
      " [-0.0009204 ]\n",
      " [-0.00199256]\n",
      " ...\n",
      " [-0.00048431]\n",
      " [-0.00309708]\n",
      " [-0.00064046]]\n",
      "grad_loss [[-0.00106285]\n",
      " [-0.00092003]\n",
      " [-0.00199218]\n",
      " ...\n",
      " [-0.00048393]\n",
      " [-0.00309671]\n",
      " [-0.00064009]]\n",
      "grad_loss [[-0.00106247]\n",
      " [-0.00091965]\n",
      " [-0.0019918 ]\n",
      " ...\n",
      " [-0.00048355]\n",
      " [-0.00309633]\n",
      " [-0.00063971]]\n",
      "grad_loss [[-0.00106209]\n",
      " [-0.00091926]\n",
      " [-0.00199141]\n",
      " ...\n",
      " [-0.00048316]\n",
      " [-0.00309594]\n",
      " [-0.00063932]]\n",
      "grad_loss [[-0.00106169]\n",
      " [-0.00091886]\n",
      " [-0.00199101]\n",
      " ...\n",
      " [-0.00048276]\n",
      " [-0.00309554]\n",
      " [-0.00063892]]\n",
      "grad_loss [[-0.00106127]\n",
      " [-0.00091845]\n",
      " [-0.0019906 ]\n",
      " ...\n",
      " [-0.00048235]\n",
      " [-0.00309513]\n",
      " [-0.00063851]]\n",
      "grad_loss [[-0.00106084]\n",
      " [-0.00091801]\n",
      " [-0.00199016]\n",
      " ...\n",
      " [-0.00048191]\n",
      " [-0.00309469]\n",
      " [-0.00063807]]\n",
      "grad_loss [[-0.00106037]\n",
      " [-0.00091755]\n",
      " [-0.0019897 ]\n",
      " ...\n",
      " [-0.00048145]\n",
      " [-0.00309423]\n",
      " [-0.00063761]]\n",
      "grad_loss [[-0.00105986]\n",
      " [-0.00091704]\n",
      " [-0.00198919]\n",
      " ...\n",
      " [-0.00048094]\n",
      " [-0.00309372]\n",
      " [-0.0006371 ]]\n",
      "grad_loss [[-0.00105929]\n",
      " [-0.00091647]\n",
      " [-0.00198862]\n",
      " ...\n",
      " [-0.00048037]\n",
      " [-0.00309315]\n",
      " [-0.00063653]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#solve_XOR()\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mpredict_taxi_trip_duration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 65\u001b[0m, in \u001b[0;36mpredict_taxi_trip_duration\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m mse_loss \u001b[38;5;241m=\u001b[39m MSELoss()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training Taxi Trip Duration Network ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mX_train_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Plot training and validation loss curves\u001b[39;00m\n\u001b[0;32m     71\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mtrain_regression\u001b[1;34m(network, loss_layer, X_train, y_train, X_val, y_val, epochs, lr, patience)\u001b[0m\n\u001b[0;32m      5\u001b[0m patience_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# --- Training Step ---\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     pred_train \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m loss_layer\u001b[38;5;241m.\u001b[39mforward(pred_train, y_train)\n\u001b[0;32m     10\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #solve_XOR()\n",
    "    predict_taxi_trip_duration()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
